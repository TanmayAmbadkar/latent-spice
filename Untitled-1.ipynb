{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'cursor_in_leading_ws' could not be imported from '/scratch1/tsa5252/anaconda3/envs/spiceenv/lib/python3.8/site-packages/IPython/terminal/shortcuts/__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.int = np.int64\n",
    "np.float = np.float64\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import itertools\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_soft_actor_critic.replay_memory import ReplayMemory\n",
    "\n",
    "from benchmarks import envs\n",
    "from e2c.env_model import get_environment_model\n",
    "from src.policy import Shield, SACPolicy, ProjectionPolicy, CSCShield\n",
    "from abstract_interpretation import domains, verification\n",
    "import gymnasium as gym\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='SPICE Args')\n",
    "parser.add_argument('--env_name', default=\"lunar_lander_R\",\n",
    "                    help='Environment (default: acc)')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='Policy Type: Gaussian | Deterministic (default: Gaussian)')\n",
    "parser.add_argument('--eval', type=bool, default=True,\n",
    "                    help='Evaluates a policy a policy every few episodes (default: True)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient (tau) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter alpha determines the relative importance of the entropy\\\n",
    "                            term against the reward (default: 0.2)')\n",
    "parser.add_argument('--automatic_entropy_tuning', default=False, action='store_true',\n",
    "                    help='Automaically adjust alpha (default: False)')\n",
    "parser.add_argument('--seed', type=int, default=123456, metavar='N',\n",
    "                    help='random seed (default: 123456)')\n",
    "parser.add_argument('--batch_size', type=int, default=1024, metavar='N',\n",
    "                    help='batch size (default: 1024)')\n",
    "parser.add_argument('--num_steps', type=int, default=10000000, metavar='N',\n",
    "                    help='maximum number of steps (default: 10000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=40, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--start_steps', type=int, default=10000, metavar='N',\n",
    "                    help='Steps sampling random actions (default: 10000)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "parser.add_argument('--cuda', action=\"store_true\",\n",
    "                    help='run on CUDA (default: False)')\n",
    "parser.add_argument('--horizon', type=int, default=5,\n",
    "                    help='The safety horizon')\n",
    "parser.add_argument('--neural_safety', default=False, action='store_true',\n",
    "                    help='Use a neural safety signal')\n",
    "parser.add_argument('--neural_threshold', type=float, default=0.1,\n",
    "                    help='Safety threshold for the neural model')\n",
    "parser.add_argument('--red_dim', type=int, default=4,\n",
    "                    help='Reduced dimension size')\n",
    "parser.add_argument('--no_safety', default=False, action='store_true',\n",
    "                    help='To use safety or no safety')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Arguments:\")\n",
    "print(args)\n",
    "hyperparams = vars(args)\n",
    "\n",
    "\n",
    "env = envs.get_env_from_name(args.env_name)\n",
    "env.seed(args.seed)\n",
    "\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Agent\n",
    "# agent = SACPolicy(env, args.replay_size, args.seed, args.batch_size, args)\n",
    "# safe_agent = None\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter('runs/temp/{}_SAC_{}_{}_{}'.format(\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), args.env_name,\n",
    "    args.policy, \"autotune\" if args.automatic_entropy_tuning else \"\"))\n",
    "\n",
    "print(hyperparams)\n",
    "if not os.path.exists(\"logs\"):\n",
    "    os.makedirs(\"logs\")\n",
    "\n",
    "file = open('logs/temp/{}_SAC_{}_{}_{}.txt'.format(\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), args.env_name,\n",
    "    args.policy, \"autotune\" if args.automatic_entropy_tuning else \"\"), \"w+\")\n",
    "\n",
    "# Memory\n",
    "real_data = ReplayMemory(args.replay_size, env.observation_space, env.action_space.shape[0], args.seed)\n",
    "e2c_data = ReplayMemory(args.replay_size, env.observation_space, env.action_space.shape[0], args.seed)\n",
    "\n",
    "real_unsafe_episodes = 0\n",
    "total_real_episodes = 0\n",
    "total_numsteps = 0\n",
    "\n",
    "update_steps = 10\n",
    "\n",
    "\n",
    "iterator_loop = itertools.count(1)\n",
    "\n",
    "agent = SACPolicy(env, args.replay_size, args.seed, args.batch_size, args)\n",
    "\n",
    "\n",
    "# PRETRAINING \n",
    "original_obs_space = env.observation_space\n",
    "updates = 0\n",
    "while total_numsteps < args.start_steps:\n",
    "    \n",
    "    i_episode = next(iterator_loop)\n",
    "    state, _ = env.reset()\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    print(i_episode, \": Real data\")\n",
    "    real_buffer = []\n",
    "    done = False\n",
    "    trunc = False\n",
    "    while not done and not trunc :\n",
    "        \n",
    "        if not args.no_safety:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent(state)\n",
    "        # if len(agent.memory) > args.batch_size and args.no_safety:\n",
    "        if total_numsteps % update_steps == 0  and len(agent.memory) > args.batch_size and args.no_safety:\n",
    "            # Number of updates per step in environment\n",
    "            for i in range(args.updates_per_step):\n",
    "                # Update parameters of all the networks\n",
    "                critic_1_loss, critic_2_loss, policy_loss, ent_l, alph = \\\n",
    "                    agent.train()\n",
    "\n",
    "                writer.add_scalar(f'loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar(f'loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar(f'loss/policy', policy_loss, updates)\n",
    "                writer.add_scalar(f'loss/entropy_loss', ent_l, updates)\n",
    "                writer.add_scalar(f'loss/alpha', alph, updates)\n",
    "                updates += 1\n",
    "\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        cost = 0\n",
    "        if env.unsafe(next_state, False):\n",
    "            real_unsafe_episodes += 1\n",
    "            episode_reward -= 10\n",
    "            reward+=-10\n",
    "            print(\"UNSAFE (outside testing)\", np.round(next_state, 2))\n",
    "            done = True\n",
    "            cost = 1\n",
    "\n",
    "        real_buffer.append((state, action, reward, next_state, done,\n",
    "            cost))\n",
    "\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    \n",
    "    for (state, action, reward, next_state, mask, cost) in real_buffer:\n",
    "        if cost > 0:\n",
    "            real_data.push(state, action, reward, next_state, mask, 1)\n",
    "        else:\n",
    "            real_data.push(state, action, reward, next_state, mask, 0)\n",
    "            \n",
    "    for (state, action, reward, next_state, mask, cost) in real_buffer:\n",
    "        if cost > 0:\n",
    "            agent.add(state, action, reward, next_state, mask, 1)\n",
    "        else:\n",
    "            agent.add(state, action, reward, next_state, mask, 0)\n",
    "\n",
    "    # total_episodes += 1 \n",
    "    \n",
    "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\"\n",
    "          .format(i_episode, total_numsteps,\n",
    "                  episode_steps, round(episode_reward, 2)))\n",
    "    \n",
    "    total_real_episodes += 1\n",
    "\n",
    "if not args.no_safety:     \n",
    "    states, actions, rewards, next_states, dones, costs = \\\n",
    "        real_data.sample(len(real_data), get_cost=True, remove_samples = True)\n",
    "\n",
    "    for (state, action, reward, next_state, mask, cost) in zip(states, actions, rewards, next_states, dones, costs):\n",
    "        e2c_data.push(state, action, reward, next_state, mask, cost)\n",
    "\n",
    "\n",
    "    if args.neural_safety:\n",
    "        env_model, cost_model = get_environment_model(\n",
    "                states, actions, next_states, rewards, costs,\n",
    "                torch.tensor(np.concatenate([env.observation_space.low, env.action_space.low])),\n",
    "                torch.tensor(np.concatenate([env.observation_space.high, env.action_space.high])),\n",
    "                model_pieces=20, seed=args.seed, policy=agent,\n",
    "                use_neural_model=False, cost_model=None, e2c_predictor = None, latent_dim=args.red_dim)\n",
    "    else:\n",
    "        env_model, cost_model = get_environment_model(\n",
    "                states, actions, next_states, rewards, costs,\n",
    "                domains.DeepPoly(env.observation_space.low, env.observation_space.high),\n",
    "                model_pieces=20, seed=args.seed, policy=None,\n",
    "                use_neural_model=False, cost_model=None, e2c_predictor = None, latent_dim=args.red_dim, horizon = args.horizon)\n",
    "\n",
    "        \n",
    "    new_obs_space = domains.DeepPoly(*verification.get_variational_bounds(env_model.mars.e2c_predictor, domains.DeepPoly(env.observation_space.low, env.observation_space.high)))\n",
    "    env.observation_space = gym.spaces.Box(low=new_obs_space.lower.detach().numpy(), high=new_obs_space.upper.detach().numpy(), shape=(args.red_dim,))\n",
    "    agent = SACPolicy(env, args.replay_size, args.seed, args.batch_size, args)\n",
    "    \n",
    "    env.safety = domains.DeepPoly(*verification.get_variational_bounds(env_model.mars.e2c_predictor, env.original_safety))\n",
    "    \n",
    "    \n",
    "    unsafe_domains_list = [verification.get_variational_bounds(env_model.mars.e2c_predictor, unsafe_dom) for unsafe_dom in env.unsafe_domains]\n",
    "    unsafe_domains_list = [domains.DeepPoly(*unsafe_dom) for unsafe_dom in unsafe_domains_list]\n",
    "        \n",
    "    \n",
    "    domain = verification.get_constraints(env_model.mars.e2c_predictor.encoder.shared_net, domains.DeepPoly(env.original_observation_space.low, env.original_observation_space.high))\n",
    "    mu_domain = verification.get_constraints(env_model.mars.e2c_predictor.encoder.fc_mu, domain)\n",
    "    recovered_dom = verification.get_constraints(env_model.mars.e2c_predictor.decoder.net, mu_domain)\n",
    "    print(\"Recovered domain\", recovered_dom.calculate_bounds())\n",
    "    print(\"Original domain\",env.original_observation_space)\n",
    "    \n",
    "    \n",
    "    print(unsafe_domains_list)\n",
    "    print(\"SAFETY: \", env.safety)\n",
    "    print(\"OBS SPACE: \", env.observation_space)\n",
    "    \n",
    "    polys = [np.array(env.safety.to_hyperplanes())]\n",
    "\n",
    "    env.safe_polys = polys\n",
    "    env.state_processor = env_model.mars.e2c_predictor.transform\n",
    "    env.polys = [np.array(domain.to_hyperplanes()) for domain in unsafe_domains_list]\n",
    "    \n",
    "    if args.neural_safety:\n",
    "        safe_agent = CSCShield(agent, cost_model,\n",
    "                                threshold=args.neural_threshold)\n",
    "    else:\n",
    "        shield = ProjectionPolicy(\n",
    "            env_model.get_symbolic_model(), env.observation_space,\n",
    "            env.action_space, args.horizon, env.polys, env.safe_polys)\n",
    "        safe_agent = Shield(shield, agent)\n",
    "\n",
    "    # Push collected training data to agent\n",
    "\n",
    "\n",
    "    # for (state, action, rewards, next_state, mask, cost) in zip(states, actions, rewards, next_states, dones, costs):\n",
    "    #     agent.add(env_model.mars.e2c_predictor.transform(state[0]), action[0], rewards[0], env_model.mars.e2c_predictor.transform(next_state[0]), mask[0], cost[0])\n",
    "\n",
    "else:\n",
    "    safe_agent=None\n",
    "    \n",
    "    \n",
    "next_states = []\n",
    "predictions = []\n",
    "\n",
    "rewards_true = []\n",
    "rewards_pred = []\n",
    "\n",
    "# Test env loop\n",
    "for i in range(10):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "\n",
    "    while not done and not trunc:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward_true, done, trunc, info = env.step(action)\n",
    "        \n",
    "        \n",
    "        next_state_pred, reward_pred= env_model(state, action, use_neural_model=False)\n",
    "        \n",
    "        if i == 1:\n",
    "            print(\"Next state\", next_state)\n",
    "            print(\"Next state pred\", next_state_pred)\n",
    "            # print(\"Reward true\", reward_true)\n",
    "            # print(\"Reward pred\", reward_pred)\n",
    "            # print(f\"True state {np.round(info['state_original'], 3)}\")\n",
    "            # print(f\"Predicted state {np.round(env_model.mars.e2c_predictor.inverse_transform(next_state), 3)}\")\n",
    "        \n",
    "        state = next_state_pred\n",
    "        \n",
    "        predictions.append(next_state_pred)\n",
    "        next_states.append(next_state)\n",
    "        \n",
    "        # if i == 1:\n",
    "            # print(\"TRUE UNSAFE\", env.unsafe(info['state_original'], False))\n",
    "            # print(\"Predicted UNSAFE\", env.unsafe(next_state_pred, True))\n",
    "        \n",
    "        if env.unsafe(info['state_original'], False):\n",
    "            print(\"UNSAFE\")\n",
    "            break\n",
    "        \n",
    "        rewards_pred.append(reward_pred)\n",
    "        rewards_true.append(reward_true)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "next_states = np.array(next_states)\n",
    "rewards_pred = np.array(rewards_pred)\n",
    "rewards_true = np.array(rewards_true)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(np.vstack([next_states, predictions.reshape(-1, args.red_dim)]))\n",
    "# X_2d = X_2d[:len(next_states)]\n",
    "plt.scatter(X_2d[:len(next_states), 0], X_2d[:len(next_states), 1], c='r', label='True')\n",
    "plt.savefig(\"scatter_e2c.png\")\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "# X_2d = tsne.fit_transform(predictions.reshape(-1, args.red_dim))\n",
    "# X_2d = tsne.transform(predictions.reshape(-1, args.red_dim))\n",
    "plt.scatter(X_2d[len(next_states):, 0], X_2d[len(next_states):, 1], c='g', label='Pred')\n",
    "plt.savefig(\"scatter_e2c_pred.png\")\n",
    "\n",
    "print(\"MSE, MAE for states\", np.mean((predictions - next_states)**2), np.mean(np.abs(predictions - next_states)))\n",
    "print(\"MSE, MAE for rewards\", np.mean((rewards_pred - rewards_true)**2), np.mean(np.abs(rewards_pred - rewards_true)))\n",
    "\n",
    "print(\"r2 score\", r2_score(predictions.reshape(predictions.shape[0], -1), next_states.reshape(next_states.shape[0], -1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiceenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
